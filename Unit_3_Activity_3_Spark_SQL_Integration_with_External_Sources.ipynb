{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUpg_lmxfBgN"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.window import Window"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# 1. Spark Session Configuration\n",
        "# ---------------------------------------------------------\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Spark SQL External Integration\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
        "    .config(\"spark.sql.autoBroadcastJoinThreshold\", 10 * 1024 * 1024) \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "spark.sparkContext.setLogLevel(\"WARN\")"
      ],
      "metadata": {
        "id": "BxKJiL_ofHBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jdbc_url = \"jdbc:mysql://localhost:3306/salesdb\"\n",
        "connection_properties = {\n",
        "    \"user\": \"root\",\n",
        "    \"password\": \"password\",\n",
        "    \"driver\": \"com.mysql.cj.jdbc.Driver\"\n",
        "}\n",
        "\n",
        "# Load tables from MySQL\n",
        "customers_df = spark.read.jdbc(\n",
        "    url=jdbc_url,\n",
        "    table=\"customers\",\n",
        "    properties=connection_properties\n",
        ")\n",
        "\n",
        "orders_df = spark.read.jdbc(\n",
        "    url=jdbc_url,\n",
        "    table=\"orders\",\n",
        "    properties=connection_properties\n",
        ")\n",
        "\n",
        "customers_df.printSchema()\n",
        "orders_df.printSchema()\n"
      ],
      "metadata": {
        "id": "L-4o_E1-fKZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s3_sales_df = spark.read \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .csv(\"s3a://my-bucket/sales_data/\")\n"
      ],
      "metadata": {
        "id": "r7W8ab4cfNKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Register Temp Views\n",
        "customers_df.createOrReplaceTempView(\"customers\")\n",
        "orders_df.createOrReplaceTempView(\"orders\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Advanced SQL with CTE, JOIN, Window Function\n",
        "# ---------------------------------------------------------\n",
        "query = \"\"\"\n",
        "WITH ranked_orders AS (\n",
        "    SELECT\n",
        "        c.customer_id,\n",
        "        c.customer_name,\n",
        "        o.order_id,\n",
        "        o.order_date,\n",
        "        o.amount,\n",
        "        SUM(o.amount) OVER (\n",
        "            PARTITION BY c.customer_id\n",
        "            ORDER BY o.order_date\n",
        "            ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n",
        "        ) AS running_total,\n",
        "        RANK() OVER (\n",
        "            PARTITION BY c.customer_id\n",
        "            ORDER BY o.amount DESC\n",
        "        ) AS order_rank\n",
        "    FROM customers c\n",
        "    JOIN orders o\n",
        "        ON c.customer_id = o.customer_id\n",
        ")\n",
        "SELECT *\n",
        "FROM ranked_orders\n",
        "WHERE order_rank <= 3\n",
        "ORDER BY customer_id, order_date\n",
        "\"\"\"\n",
        "\n",
        "result_df = spark.sql(query)\n",
        "result_df.show(truncate=False)\n"
      ],
      "metadata": {
        "id": "_yZ8sht3fPA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_df.explain(True)\n"
      ],
      "metadata": {
        "id": "MMpiSpDQfR0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import broadcast\n",
        "\n",
        "optimized_df = orders_df.join(\n",
        "    broadcast(customers_df),\n",
        "    \"customer_id\",\n",
        "    \"inner\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "DEDlXd8LfTT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.conf.set(\"spark.sql.optimizer.dynamicPartitionPruning\", \"true\")\n"
      ],
      "metadata": {
        "id": "3YH0A4Q3fUxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# Save Output with Partitioning and Compression\n",
        "# ---------------------------------------------------------\n",
        "output_path = \"output/customer_sales_summary\"\n",
        "\n",
        "result_df.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .partitionBy(\"customer_id\") \\\n",
        "    .option(\"compression\", \"snappy\") \\\n",
        "    .parquet(output_path)\n"
      ],
      "metadata": {
        "id": "xDbrCkTofXBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.read.parquet(output_path).show(5)\n"
      ],
      "metadata": {
        "id": "V6mMMtH4fecG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}